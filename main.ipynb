{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "19bf0282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cleantext\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import ast\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as t\n",
    "import torchvision.models as mm\n",
    "import skimage\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4771a9f",
   "metadata": {},
   "source": [
    "## Task 1: Sentence level Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "6625ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocess(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.df=df\n",
    "        self.text=self.preprocess(df['raw'])\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self,ind):\n",
    "        text=self.text[ind]\n",
    "        return text\n",
    "    def preprocess(self,text):\n",
    "        tokenize=get_tokenizer('basic_english')\n",
    "        tt=[]\n",
    "        for i in text:\n",
    "            tt.append(cleantext.clean(str(i),clean_all=True,stemming=True,stopwords=True,lowercase=True,numbers=True,punct=True))\n",
    "        vectorizer=TfidfVectorizer(tokenizer=tokenize)\n",
    "        embeddings=vectorizer.fit_transform(tt)\n",
    "        emb=[]\n",
    "        for i in embeddings:\n",
    "            emb.append(torch.from_numpy(i.toarray()[0]))\n",
    "        padded=pad_sequence(emb,batch_first=True)\n",
    "        return padded\n",
    "imagesname=[]\n",
    "path=r'C:\\Users\\HP\\Downloads\\sentiment\\sentiment.csv'\n",
    "for i in os.listdir(r\"C:/Users/HP/Downloads/sentiment/sentiment_images\"):\n",
    "    imagesname.append(i)\n",
    "df=pd.read_csv(path)\n",
    "df=df[df['filename'].isin(imagesname)]\n",
    "df=df.reset_index()\n",
    "dataset=TextPreprocess(df)\n",
    "dataloader=DataLoader(dataset,batch_size=32,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "9e352b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning Processed Text to a Dataframe\n",
    "textfeatures = []\n",
    "with torch.no_grad():\n",
    "    for i in dataloader:\n",
    "        for j in i:\n",
    "            textfeatures.append(j.tolist())\n",
    "df['Processed Text']=textfeatures\n",
    "textfeatures=torch.tensor(textfeatures)\n",
    "sent_train=df['sentiment']\n",
    "sent_test=df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "fa52afb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#Image Preprocessing and Feature Extraction using Vgg\n",
    "features=[]\n",
    "path=r\"C:/Users/HP/Downloads/sentiment/\"\n",
    "preprocess=t.Compose([t.Resize((224,224)),t.ToTensor(),t.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])])\n",
    "dataset=datasets.ImageFolder(path,preprocess)\n",
    "dataloader=torch.utils.data.DataLoader(dataset,batch_size=32,shuffle=False)\n",
    "vgg=mm.vgg16(pretrained=True)\n",
    "with torch.no_grad():\n",
    "    for i,_ in dataloader:\n",
    "        feature=vgg(i)\n",
    "        features.append(feature)\n",
    "features=torch.cat(features,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "ee52061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping Features to its respective FileNames in the df\n",
    "ii=[]\n",
    "for i in os.listdir(r\"C:/Users/HP/Downloads/sentiment/sentiment_images\"):\n",
    "    ii.append(i)\n",
    "f=[]\n",
    "for i in df['filename']:\n",
    "    for j in range(0,len(ii)):\n",
    "        if i==ii[j]:\n",
    "            f.append(features[j])\n",
    "df['features']=f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "466ad6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test_Train Split\n",
    "df1=df[(df['split']=='train')]\n",
    "df1=df1.reset_index()\n",
    "df2=df[(df['split']=='val')]\n",
    "df2=df2.reset_index()\n",
    "df3=df[(df['split']=='test')]\n",
    "df3=df3.reset_index()\n",
    "image_train=df1['features']\n",
    "text_train=df1['Processed Text']\n",
    "sent_train=df1['sentiment']\n",
    "imagevalfeatures=df2['features']\n",
    "textval=df3['Processed Text']\n",
    "sent_val=df2['sentiment']\n",
    "imagetestfeatures=df3['features']\n",
    "texttest=df3['Processed Text']\n",
    "sent_test=df3['sentiment']\n",
    "image_train=list(image_train)\n",
    "imagefeatures=torch.stack(image_train)\n",
    "text_train=list(text_train)\n",
    "textfeatures=torch.tensor(text_train)\n",
    "textval=list(textval)\n",
    "textval=torch.tensor(textval)\n",
    "imagevalfeatures=list(imagevalfeatures)\n",
    "imagevalfeatures=torch.stack(imagevalfeatures)\n",
    "imagetestfeatures=list(imagetestfeatures)\n",
    "imagetestfeatures=torch.stack(imagetestfeatures)\n",
    "texttest=list(texttest)\n",
    "texttest=torch.tensor(texttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "f4ec9dee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creating a Dataloader for both image and text features\n",
    "class Text_Image:\n",
    "    def __init__(self,image,text,sent):\n",
    "        self.image=image\n",
    "        self.text=text\n",
    "        self.sent=sent\n",
    "    def __len__(self):\n",
    "        return len(self.image)\n",
    "    def __getitem__(self,index):\n",
    "        image=self.image[index]\n",
    "        text=self.text[index]\n",
    "        sent=self.sent[index]\n",
    "        return image,text,sent\n",
    "dataset=Text_Image(imagefeatures,textfeatures,sent_train)\n",
    "CombineDataloader=DataLoader(dataset,batch_size=43,shuffle=False)\n",
    "dataset=Text_Image(imagevalfeatures,textval,sent_val)\n",
    "valDataloader=DataLoader(dataset,batch_size=43,shuffle=False)\n",
    "class Test:\n",
    "    def __init__(self,image,text,sent):\n",
    "        self.image=image\n",
    "        self.text=text\n",
    "        self.sent=sent\n",
    "    def __len__(self):\n",
    "        return len(self.image)\n",
    "    def __getitem__(self,index):\n",
    "        image=self.image[index]\n",
    "        text=self.text[index]\n",
    "        sent=self.sent[index]\n",
    "        return image,text,sent\n",
    "testdataset=Test(imagetestfeatures,texttest,sent_test)\n",
    "testCombineDataloader=DataLoader(testdataset,batch_size=43,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "b41beb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MultiModal Architecture\n",
    "class imagemodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.linear1=nn.Linear(1000,800)\n",
    "        self.act_fn=nn.Sigmoid()\n",
    "        self.linear2=nn.Linear(800,640)\n",
    "        self.linear3=nn.Linear(640,320)\n",
    "        self.linear4=nn.Linear(320,256)\n",
    "    def forward(self,x):\n",
    "        x=self.flatten(x)\n",
    "        x=self.linear1(x)\n",
    "        x=self.act_fn(x)\n",
    "        x=self.linear2(x)\n",
    "        x=self.act_fn(x)\n",
    "        x=self.linear3(x)\n",
    "        x=self.act_fn(x)\n",
    "        x=self.linear4(x)\n",
    "        x=self.act_fn(x)\n",
    "        return x\n",
    "class textmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.linear1=nn.Linear(3324,800)\n",
    "        self.act_fn=nn.Sigmoid()\n",
    "        self.linear2=nn.Linear(800,640)\n",
    "        self.linear3=nn.Linear(640,320)\n",
    "        self.linear4=nn.Linear(320,256)\n",
    "    def forward(self,x):\n",
    "        x=self.flatten(x)\n",
    "        x=self.linear1(x)\n",
    "        x=self.act_fn(x)\n",
    "        x=self.linear2(x)\n",
    "        x=self.act_fn(x)\n",
    "        x=self.linear3(x)\n",
    "        x=self.act_fn(x)\n",
    "        x=self.linear4(x)\n",
    "        x=self.act_fn(x)\n",
    "        return x\n",
    "class fuse(nn.Module):\n",
    "    def __init__(self,textsmodel,imagesmodel,inputsize):\n",
    "        super().__init__()\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.imagemodel=imagesmodel\n",
    "        self.textmodel=textsmodel\n",
    "        self.linear1=nn.Linear(inputsize,256)\n",
    "        self.linear2=nn.Linear(256,128)\n",
    "        self.linear3=nn.Linear(128,80)\n",
    "        self.linear4=nn.Linear(80,64)\n",
    "        self.linear5=nn.Linear(64,32)\n",
    "        self.linear6=nn.Linear(32,16)\n",
    "        self.linear7=nn.Linear(16,8)\n",
    "        self.linear8=nn.Linear(8,2)\n",
    "        self.act_fn=nn.Sigmoid() \n",
    "    def forward(self,images,texts):\n",
    "        texts=self.textmodel(texts.float())\n",
    "        images=self.imagemodel(images.float())\n",
    "        o=torch.cat((texts,images),dim=1)\n",
    "        self.flatten=nn.Flatten()\n",
    "        output=self.linear1(o)\n",
    "        output=self.act_fn(output)\n",
    "        output=self.linear2(output)\n",
    "        output=self.act_fn(output)\n",
    "        output=self.linear3(output)\n",
    "        output=self.act_fn(output)\n",
    "        output=self.linear4(output)\n",
    "        output=self.act_fn(output)\n",
    "        output=self.linear5(output)\n",
    "        output=self.act_fn(output)\n",
    "        output=self.linear6(output)\n",
    "        output=self.act_fn(output)\n",
    "        output=self.linear7(output)\n",
    "        output=self.act_fn(output)\n",
    "        output=self.linear8(output)\n",
    "        output=self.act_fn(output)\n",
    "        return output\n",
    "imagesmodel=imagemodel()\n",
    "textsmodel=textmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "0ec2c6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1\n",
      "Training Loss : 309.81534814834595\n",
      "Validation Loss:  62.863512337207794\n",
      "Epoch :  2\n",
      "Training Loss : 309.8139919042587\n",
      "Validation Loss:  62.86428481340408\n",
      "Epoch :  3\n",
      "Training Loss : 309.81359881162643\n",
      "Validation Loss:  62.86484855413437\n",
      "Epoch :  4\n",
      "Training Loss : 309.8135393857956\n",
      "Validation Loss:  62.865243792533875\n",
      "Epoch :  5\n",
      "Training Loss : 309.8135803937912\n",
      "Validation Loss:  62.865494310855865\n",
      "Epoch :  6\n",
      "Training Loss : 309.81364530324936\n",
      "Validation Loss:  62.86566388607025\n",
      "Epoch :  7\n",
      "Training Loss : 309.8137018084526\n",
      "Validation Loss:  62.86576807498932\n",
      "Epoch :  8\n",
      "Training Loss : 309.81374484300613\n",
      "Validation Loss:  62.86584252119064\n",
      "Epoch :  9\n",
      "Training Loss : 309.813774228096\n",
      "Validation Loss:  62.865888476371765\n",
      "Epoch :  10\n",
      "Training Loss : 309.8137977719307\n",
      "Validation Loss:  62.865917563438416\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "model=fuse(textsmodel,imagesmodel,256+256)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "vc=nn.CrossEntropyLoss()\n",
    "validationoptimizer=torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "i=0\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    trainloss=0.0\n",
    "    i=i+1\n",
    "    for images,texts,labels in CombineDataloader:\n",
    "        outputs=model(images,texts)\n",
    "        loss=criterion(outputs,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trainloss=trainloss+loss.item()\n",
    "    model.eval()\n",
    "    vloss=0.0\n",
    "    with torch.no_grad():\n",
    "        for images,texts,labels in valDataloader:\n",
    "            validationoptimizer.zero_grad()\n",
    "            outputs = model(images, texts)\n",
    "            loss=vc(outputs,labels)\n",
    "            vloss=vloss+loss.item()\n",
    "    print('Epoch : ',i)\n",
    "    print('Training Loss :',trainloss)\n",
    "    print('Validation Loss: ',vloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "32a310ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 54.807692307692314\n",
      "Recall : 1.0\n",
      "Precision : 0.5480769230769231\n",
      "F1 Score : 0.7080745341614907\n"
     ]
    }
   ],
   "source": [
    "#Saving the Model\n",
    "torch.save(model.state_dict(),\"trained_model.pth\")\n",
    "#Evaluate\n",
    "model=fuse(textsmodel,imagesmodel,256+256)\n",
    "model.load_state_dict(torch.load(\"trained_model.pth\"))\n",
    "model.eval()\n",
    "pred=[]\n",
    "label=[]\n",
    "with torch.no_grad():\n",
    "    for images,texts,labels in testCombineDataloader:\n",
    "        outputs=model(images,texts)\n",
    "        for i,j in zip(torch.argmax(outputs,axis=1),labels):\n",
    "            pred.append(i)\n",
    "            label.append(j)\n",
    "accuracy=metrics.accuracy_score(label,pred)\n",
    "precision=metrics.precision_score(label,pred)\n",
    "recall=metrics.recall_score(label,pred)\n",
    "f1score=metrics.f1_score(label,pred)\n",
    "print('Accuracy :',accuracy*100)\n",
    "print('Recall :',recall)\n",
    "print('Precision :',precision)\n",
    "print('F1 Score :',f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "4f806f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  255.50272405147552\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "torch.save(model.state_dict(),\"trained_model.pth\")\n",
    "model=fuse(textsmodel,imagesmodel,256+256)\n",
    "criteria=nn.CrossEntropyLoss()\n",
    "model.load_state_dict(torch.load(\"trained_model.pth\"))\n",
    "model.eval()\n",
    "loss=0.0\n",
    "with torch.no_grad():\n",
    "    for images,texts,labels in testCombineDataloader:\n",
    "        outputs=model(images,texts)\n",
    "        losss=criteria(outputs,labels)\n",
    "        loss=loss+losss.item()\n",
    "print(\"Test Loss: \",loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf35fe",
   "metadata": {},
   "source": [
    "## Task 2: Word level Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f94e5268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Bigrams\n",
    "class textbigrams:\n",
    "    def __init__(self,df):\n",
    "        self.df=df\n",
    "        self.text=self.preprocess(df['raw'])\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self,ind):\n",
    "        text=self.text[ind]\n",
    "        return text\n",
    "    def preprocess(self,text):\n",
    "        tt=[]\n",
    "        for i in text:\n",
    "            tt.append(cleantext.clean(str(i),clean_all=True,stemming=True,stopwords=True,lowercase=True,numbers=True,punct=True))\n",
    "        bigram=[]\n",
    "        for i in tt:\n",
    "            j=word_tokenize(i)\n",
    "            bigrams=list(ngrams(j,2))\n",
    "            bigram.append(bigrams)\n",
    "        index={}\n",
    "        for i in bigram:\n",
    "            for j in i:\n",
    "                if j not in index:\n",
    "                    index[j]=len(index)\n",
    "        embedded=[]\n",
    "        for i in bigram:\n",
    "            emb=torch.tensor([index[j] for j in i])\n",
    "            embedded.append(emb)\n",
    "        padded=pad_sequence(embedded,batch_first=True,padding_value=0)\n",
    "        return padded\n",
    "imagesname=[]\n",
    "df = pd.read_csv(r'C:\\Users\\HP\\Downloads\\sentiment\\sentiment.csv')\n",
    "for i in os.listdir(r\"C:/Users/HP/Downloads/sentiment/sentiment_images\"):\n",
    "    imagesname.append(i)\n",
    "df=df[df['filename'].isin(imagesname)]\n",
    "df=df.reset_index()\n",
    "df1=df[(df['split']=='train')]\n",
    "df1=df1.reset_index()\n",
    "dataset=textbigrams(df1)\n",
    "dataloader=DataLoader(dataset,batch_size=32,shuffle=False)\n",
    "bigramfeatures=[]\n",
    "with torch.no_grad():\n",
    "    for i in dataloader:\n",
    "        for j in i:\n",
    "            bigramfeatures.append(j.tolist())\n",
    "bigramfeatures=torch.tensor(bigramfeatures)\n",
    "trainsent=df1['word_sentiment'].apply(lambda i:[int(j) for j in ast.literal_eval(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "abe8351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Text Bigrams\n",
    "class testtextbigrams:\n",
    "    def __init__(self,df):\n",
    "        self.df=df\n",
    "        self.text=self.preprocess(df['raw'])\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self,ind):\n",
    "        text=self.text[ind]\n",
    "        return text\n",
    "    def preprocess(self,text):\n",
    "        tt=[]\n",
    "        for i in text:\n",
    "            tt.append(cleantext.clean(str(i),clean_all=True,stemming=True,stopwords=True,lowercase=True,numbers=True,punct=True))\n",
    "        bigram=[]\n",
    "        for i in tt:\n",
    "            j=word_tokenize(i)\n",
    "            bigrams=list(ngrams(j,2))\n",
    "            bigram.append(bigrams)\n",
    "        index={}\n",
    "        for i in bigram:\n",
    "            for j in i:\n",
    "                if j not in index:\n",
    "                    index[j]=len(index)\n",
    "        embedded=[]\n",
    "        for i in bigram:\n",
    "            emb=torch.tensor([index[j] for j in i])\n",
    "            embedded.append(emb)\n",
    "        padded=pad_sequence(embedded,batch_first=True,padding_value=0)\n",
    "        return padded\n",
    "df2=df[(df['split']=='test')]\n",
    "df2=df2.reset_index()\n",
    "dataset=textbigrams(df2)\n",
    "testdataloader=DataLoader(dataset,batch_size=32,shuffle=False)\n",
    "testbigramfeatures=[]\n",
    "with torch.no_grad():\n",
    "    for i in testdataloader:\n",
    "        for j in i:\n",
    "            testbigramfeatures.append(j.tolist())\n",
    "testbigramfeatures=torch.tensor(testbigramfeatures)\n",
    "testsent=df2['word_sentiment'].apply(lambda i:[int(j) for j in ast.literal_eval(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "b575f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Val Text Bigrams\n",
    "class valbigrams:\n",
    "    def __init__(self,df):\n",
    "        self.df=df\n",
    "        self.text=self.preprocess(df['raw'])\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self,ind):\n",
    "        text=self.text[ind]\n",
    "        return text\n",
    "    def preprocess(self,text):\n",
    "        tt=[]\n",
    "        for i in text:\n",
    "            tt.append(cleantext.clean(str(i),clean_all=True,stemming=True,stopwords=True,lowercase=True,numbers=True,punct=True))\n",
    "        bigram=[]\n",
    "        for i in tt:\n",
    "            j=word_tokenize(i)\n",
    "            bigrams=list(ngrams(j,2))\n",
    "            bigram.append(bigrams)\n",
    "        index={}\n",
    "        for i in bigram:\n",
    "            for j in i:\n",
    "                if j not in index:\n",
    "                    index[j]=len(index)\n",
    "        embedded=[]\n",
    "        for i in bigram:\n",
    "            emb=torch.tensor([index[j] for j in i])\n",
    "            embedded.append(emb)\n",
    "        padded=pad_sequence(embedded,batch_first=True,padding_value=0)\n",
    "        return padded\n",
    "df3=df[(df['split']=='val')]\n",
    "df3=df3.reset_index()\n",
    "dataset=valbigrams(df3)\n",
    "valdataloader=DataLoader(dataset,batch_size=32,shuffle=False)\n",
    "valbigramfeatures=[]\n",
    "with torch.no_grad():\n",
    "    for i in valdataloader:\n",
    "        for j in i:\n",
    "            valbigramfeatures.append(j.tolist())\n",
    "valbigramfeatures=torch.tensor(valbigramfeatures)\n",
    "valsent=df3['word_sentiment'].apply(lambda i:[int(j) for j in ast.literal_eval(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "81ba09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Sentiments\n",
    "testsentimentbigram=[]\n",
    "for i in testsent:\n",
    "    bigrams=list(ngrams(i,2))\n",
    "    testsentimentbigram.append(bigrams)\n",
    "testsentiment=[]\n",
    "for k in testsentimentbigram:\n",
    "    for i in k:\n",
    "        m=[]\n",
    "        for j in range(0,len(i)-1):\n",
    "            if i[j]==0 and i[j+1]==0:\n",
    "                m=[0]\n",
    "            elif i[j]==0 and i[j+1]==1:\n",
    "                m=[0]\n",
    "            elif i[j]==1 and i[j+1]==0:\n",
    "                m=[0]\n",
    "            elif i[j]==1 and i[j+1]==1:\n",
    "                m=[1]\n",
    "    testsentiment.append(m)\n",
    "sent_test=[]\n",
    "for i in testsentiment:\n",
    "    t=torch.tensor(i)\n",
    "    sent_test.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "617cc13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Val Sentiments\n",
    "valsentimentbigram=[]\n",
    "for i in valsent:\n",
    "    bigrams=list(ngrams(i,2))\n",
    "    valsentimentbigram.append(bigrams)\n",
    "valsentiment=[]\n",
    "for k in valsentimentbigram:\n",
    "    for i in k:\n",
    "        m=[]\n",
    "        for j in range(0,len(i)-1):\n",
    "            if i[j]==0 and i[j+1]==0:\n",
    "                m=[0]\n",
    "            elif i[j]==0 and i[j+1]==1:\n",
    "                m=[0]\n",
    "            elif i[j]==1 and i[j+1]==0:\n",
    "                m=[0]\n",
    "            elif i[j]==1 and i[j+1]==1:\n",
    "                m=[1]\n",
    "    valsentiment.append(m)\n",
    "sent_val=[]\n",
    "for i in valsentiment:\n",
    "    t=torch.tensor(i)\n",
    "    sent_val.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "eb82de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Sentiments\n",
    "sentimentbigram=[]\n",
    "for i in trainsent:\n",
    "    bigrams=list(ngrams(i,2))\n",
    "    sentimentbigram.append(bigrams)\n",
    "sentiment=[]\n",
    "for k in sentimentbigram:\n",
    "    for i in k:\n",
    "        m=[]\n",
    "        for j in range(0,len(i)-1):\n",
    "            if i[j]==0 and i[j+1]==0:\n",
    "                m=[0]\n",
    "            elif i[j]==0 and i[j+1]==1:\n",
    "                m=[0]\n",
    "            elif i[j]==1 and i[j+1]==0:\n",
    "                m=[0]\n",
    "            elif i[j]==1 and i[j+1]==1:\n",
    "                m=[1]\n",
    "    sentiment.append(m)\n",
    "sent_train=[]\n",
    "for i in sentiment:\n",
    "    t=torch.tensor(i)\n",
    "    sent_train.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "88570352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataLoader and Dataset\n",
    "valbigramfeatures=F.pad(valbigramfeatures,(0,45-valbigramfeatures.size(1)))\n",
    "bigramfeatures=F.pad(bigramfeatures,(0,45-bigramfeatures.size(1)))\n",
    "class Text_sent:\n",
    "    def __init__(self,text,sent):\n",
    "        self.text=text\n",
    "        self.sent=sent\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self,index):\n",
    "        text=self.text[index]\n",
    "        sent=self.sent[index]\n",
    "        return text,sent\n",
    "dataset=Text_sent(bigramfeatures,sent_train)\n",
    "CombineDataloader=DataLoader(dataset,batch_size=43,shuffle=False)\n",
    "testdataset=Text_sent(testbigramfeatures,sent_test)\n",
    "testCombineDataloader=DataLoader(testdataset,batch_size=43,shuffle=False)\n",
    "valdataset=Text_sent(valbigramfeatures,sent_val)\n",
    "valCombineDataloader=DataLoader(valdataset,batch_size=43,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9d2e74a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "class bigramtextmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1=nn.Linear(45,320)\n",
    "        self.act_fn=nn.Sigmoid()\n",
    "        self.linear2=nn.Linear(320,256)\n",
    "        self.linear3=nn.Linear(256,128)\n",
    "        self.linear4=nn.Linear(128,64)\n",
    "        self.linear5=nn.Linear(64,32)\n",
    "        self.linear6=nn.Linear(32,16)\n",
    "        self.linear7=nn.Linear(16,8)\n",
    "        self.linear8=nn.Linear(8,2)\n",
    "    def forward(self,x):\n",
    "        x=x.to(torch.float32)\n",
    "        x=self.linear1(x)\n",
    "        x=self.act_fn(x)\n",
    "        x=self.linear2(x)\n",
    "        x=self.act_fn(x)\n",
    "        x=self.linear3(x)\n",
    "        x=self.act_fn(x)\n",
    "        x=self.linear4(x)\n",
    "        x=self.act_fn(x)\n",
    "        x=self.linear5(x)\n",
    "        x=self.act_fn(x)\n",
    "        x=self.linear6(x)\n",
    "        x=self.act_fn(x)\n",
    "        x=self.linear7(x)\n",
    "        x=self.act_fn(x)\n",
    "        x=self.linear8(x)\n",
    "        return x\n",
    "tokenmodel=bigramtextmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "0a920a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss : 325.37596678733826\n",
      "Validation Loss:  60.28992587327957\n",
      "Training Loss : 288.70098972320557\n",
      "Validation Loss:  56.48875394463539\n",
      "Training Loss : 279.93030121922493\n",
      "Validation Loss:  55.41835060715675\n",
      "Training Loss : 277.70346570014954\n",
      "Validation Loss:  55.07523164153099\n",
      "Training Loss : 277.1168564558029\n",
      "Validation Loss:  54.95061707496643\n",
      "Training Loss : 276.96174651384354\n",
      "Validation Loss:  54.899831384420395\n",
      "Training Loss : 276.9222120940685\n",
      "Validation Loss:  54.87705147266388\n",
      "Training Loss : 276.91324415802956\n",
      "Validation Loss:  54.86609101295471\n",
      "Training Loss : 276.91191217303276\n",
      "Validation Loss:  54.86056697368622\n",
      "Training Loss : 276.9121915102005\n",
      "Validation Loss:  54.857705384492874\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(tokenmodel.parameters(),lr=0.001)\n",
    "criterion2=nn.CrossEntropyLoss()\n",
    "optimizer2=torch.optim.Adam(tokenmodel.parameters(),lr=0.01)\n",
    "for epoch in range(10):\n",
    "    tokenmodel.train()\n",
    "    trainloss=0.0\n",
    "    for texts,labels in CombineDataloader:\n",
    "        labels=labels.view(-1)\n",
    "        outputs=tokenmodel(texts)\n",
    "        loss=criterion(outputs,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trainloss=trainloss+loss.item()\n",
    "    tokenmodel.eval()\n",
    "    vloss=0.0\n",
    "    with torch.no_grad():\n",
    "        for texts,labels in valCombineDataloader:\n",
    "            labels=labels.view(-1)\n",
    "            optimizer2.zero_grad()\n",
    "            outputs=tokenmodel(texts)\n",
    "            loss=criterion2(outputs,labels)\n",
    "            vloss=vloss+loss.item()\n",
    "    print('Training Loss :',trainloss)\n",
    "    print('Validation Loss: ',vloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "46227634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 69.947209653092\n",
      "Recall : 0.0\n",
      "Precision : 1.0\n",
      "F1 Score : 0.41158198358109604\n"
     ]
    }
   ],
   "source": [
    "#Evaluate\n",
    "#Saving the Model\n",
    "torch.save(tokenmodel.state_dict(),\"trained_model3.pth\")\n",
    "#Evaluate\n",
    "tokenmodel=bigramtextmodel()\n",
    "tokenmodel.load_state_dict(torch.load(\"trained_model3.pth\"))\n",
    "tokenmodel.eval()\n",
    "pred=[]\n",
    "label=[]\n",
    "with torch.no_grad():\n",
    "    for texts,labels in testCombineDataloader:\n",
    "        labels=labels.view(-1)\n",
    "        outputs=tokenmodel(texts)\n",
    "        for i,j in zip(torch.argmax(outputs,axis=1),labels):\n",
    "            pred.append(i.tolist())\n",
    "            label.append(j.tolist())\n",
    "accuracy=metrics.accuracy_score(label,pred)\n",
    "precision=metrics.precision_score(label,pred,zero_division=1)\n",
    "recall=metrics.recall_score(label,pred)\n",
    "f1score=metrics.f1_score(label,pred,average='macro')\n",
    "print('Accuracy :',accuracy*100)\n",
    "print('Recall :',recall)\n",
    "print('Precision :',precision)\n",
    "print('F1 Score :',f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "708e6a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  230.1139524281025\n"
     ]
    }
   ],
   "source": [
    "#Loss\n",
    "torch.save(model.state_dict(),\"trained_model2.pth\")\n",
    "tokenmodel=bigramtextmodel()\n",
    "criteria=nn.CrossEntropyLoss()\n",
    "model.load_state_dict(torch.load(\"trained_model2.pth\"))\n",
    "tokenmodel.eval()\n",
    "loss=0.0\n",
    "with torch.no_grad():\n",
    "    for texts,labels in testCombineDataloader:\n",
    "        labels=labels.view(-1)\n",
    "        outputs=tokenmodel(texts)\n",
    "        losss=criteria(outputs,labels)\n",
    "        loss=loss+losss.item()\n",
    "print(\"Test Loss: \",loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
